{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e66002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moxitron/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from collections import deque , namedtuple \n",
    "import gym\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense , Input \n",
    "from tensorflow.keras.losses import MSE \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef937f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CONSTANTS ###########\n",
    "MEM_SIZE = 100_000         #size of the memory buffer\n",
    "GAMMA = 0.995              #discount factor\n",
    "ALPHA = 1e-3               #learning rate\n",
    "NUM_STEPS_PER_UPDATE = 4   #perform update every C timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446283bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9d9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1a1fb",
   "metadata": {},
   "source": [
    "\n",
    "Here is the psuedo code we'll be using to solve lunar lander. \n",
    "<br><br>\n",
    "<figure>\n",
    "  <img src = \"img/psuedo_code.png\" width = 90% style = \"border: thin silver solid; padding: 0px\">\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Fig 3. Deep Q-Learning with Experience Replay.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Surely the most fundamental equations in reinforcement learning are Bellman Equations.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>           \n",
    "$ \\begin{equation} Q(s,a) = R(s) + \\gamma \\max_{a'} Q(s',a')\\end{equation}$\n",
    "</center>\n",
    "<br>\n",
    "Where $Q(s,a)$ is expected return when we are at state s and take action a, and $R(s)$ is reward when get when we are at state s.\n",
    "There are $\\sum_{s,a}$ variables and for each variable we have an equation. So one way to solve this is using system of linear equations.\n",
    "<br>\n",
    "In practice it's not the most efficient way. An alternative is to initialize $Q$ function randomely. It's most likely that left-hand side doesn't match and right-hand side in the Bellman equations. So we iterate over all state action pair and assign right-hand side to $Q(s,a)$. It can be proven that doing this over and over, the series of $Q$ functions converges and results in the optimal $Q$ funtion.\n",
    "<br> \n",
    "There's an inconvenience to this. In Lunar Lander the space is infinite, hence it's practically impossible to go over all state-action pairs. Recall that neural networks can compute any function. One approach is to use NNs to estimate $Q^*$. And that's what we are gonna do.\n",
    "<br>\n",
    "\n",
    "###  Target Q network\n",
    "We train NN to minimize error given below.          \n",
    "\n",
    "$$ \\begin{equation} \\overbrace{\\underbrace{R(s) + \\gamma \\max_{a'} Q(s',a' ; w)}_{y\\,target} - Q(s,a ; w)}^{Error}\\end{equation}$$\n",
    "\n",
    "The value of $y target$ is changing, having a constantly changing target can lead to oscillations and instability. To resolve this issue we create another network, identical to $Q(s,a;w)$, $\\hat{Q} (s,a ; w^- )$ and initialize its weights, $w^-$, to be same as $w$. We are only gonna update its weights every $C$ timesteps using **soft update** given by : \n",
    " \n",
    "$$ \\begin{equation} w^- \\leftarrow \\tau w + (1 - \\tau) w^- \\end{equation} $$\n",
    "\n",
    "where $\\tau\\ll1$. We'll get into what exprience replay means. For now let's implement $Q$ and $\\hat{Q}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0212541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 13:07:32.878979: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-02 13:07:32.879283: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-02 13:07:32.880329: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "q_network = Sequential([\n",
    "    \n",
    "    Input(state_size) , \n",
    "    Dense(64 , activation=\"relu\") , \n",
    "    Dense(64 , activation=\"relu\") , \n",
    "    Dense(num_actions  , activation=\"linear\") , \n",
    "]) ; \n",
    "target_q_network = Sequential([\n",
    "    \n",
    "    Input(state_size) , \n",
    "    Dense(64 , activation=\"relu\") , \n",
    "    Dense(64 , activation=\"relu\") , \n",
    "    Dense(num_actions  , activation=\"linear\") , \n",
    "    \n",
    "]) ;\n",
    "optimizer = Adam(learning_rate=ALPHA);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63597b5",
   "metadata": {},
   "source": [
    "### Exprience replay \n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207a1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3248588d",
   "metadata": {},
   "source": [
    "### Loss function \n",
    "To implement the loss function we need to shed some light on $y\\,target$. The formula is given below and we will be using a mask to make it inline in the code. \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{equation} \n",
    "    y_j =  \n",
    "    \\begin{cases}\n",
    "        R_j \\text {episode terminates at step } j + 1 \\\\\n",
    "        R_j + \\gamma \\max_{a}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(expriences , gamma , q_network , target_q_network) :  \n",
    "    \"\"\"\n",
    "    Returns cost using MSE function. \n",
    "    ------------------------------------\n",
    "    Parameters : \n",
    "    \n",
    "        expriences : list of namedtuples\n",
    "            names are [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]. \n",
    "        \n",
    "        gamma : floating point \n",
    "            learning rate to use in Bellman Equations. \n",
    "        \n",
    "        q_network : Keras Sequential model \n",
    "            network computing Q(s,a). \n",
    "        \n",
    "        target_q_network : Keras Sequential model \n",
    "            network computing Q^(s,a). \n",
    "    \"\"\"\n",
    "    \n",
    "    states , actions , rewards ,  next_states , done_vals = expriences\n",
    "    max_qsa = tf.reduce_max(target_q_network(states) , axis=-1) \n",
    "    y_target = rewards + gamma * ( 1 - done_vals ) * max_qsa \n",
    "    q_values = q_network(states) \n",
    "    q_values = tf.gather_nd(q_values , tf.stack([tf.range(q_values.shpe[0]) , \n",
    "                                                tf.cast(actions , int32)] , axis=1))\n",
    "    \n",
    "    return MSE(y_targets , q_values) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
